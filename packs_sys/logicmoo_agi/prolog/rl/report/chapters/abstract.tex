Reinforcement Learning (RL) has been applied and proven to be successful in many domains.
However, most of current RL methods face limitations, namely, low learning efficiency, 
inability to use abstract reasoning, and inability of transfer learning to similar environments.
In order to tackle these shortcomings, we introduce a new approach called ILP(RL).
ILP(RL) learns a general concept of a valid move in an environment, called a hypothesis, using ILASP, and generates a plan for a sequence of actions to the destination using ASP.
The learnt hypotheses is highly expressive and transferable to a similar environment. 
While there are a number of past papers that attempt to incorporate symbolic representation to RL problems in order to achieve efficient learning, 
there has not been any attempt of applying ILP into a RL problem.
We examined ILP(RL) in a various simple maze environments, and show that ILP(RL) learns faster than existing RL methods.
We also show that transfer learning of the learnt hypothesis successfully improves learning on a new but similar environment.
This proof of concept approach shows potentials for this new way of learning using ILP.
Although the experiments were conducted in a simple environment, the results of the experiments are promising, and open up an avenue for future research directions.
