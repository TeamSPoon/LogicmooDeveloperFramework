\section{Motivation}
\label{sec:motivation}
Reinforcement learning (RL) is a subfield of machine learning concerned agents' learning how to behave in an environment by interacting with the environment in order to maximise the total rewards they receive.
The strength of RL is that it can be applied to many different domains where it is unknown to an agent how to perform a task. 
RL has been proven to work well in a number of complex environment, such as dynamic, real environments given sufficient learning time.
Especially together with deep learning (DL), there have been many successful applications of RL in a number of domains,
such as video games \cite{Mnih2015}, the game of Go \cite{Silver2016} and robotics \cite{Levine2015}. 

Despite these successful applications of RL, there are still a number of issues that RL has to overcome.
First, most of the RL algorithms requires large number of trial-end-error interactions with long time of training, which is also computationally expensive.
Second, most of the RL algorithms have no thought process to the decision making, and do not make use of high-level abstract reasoning, 
such as understanding symbolic representations or causal reasoning.
Third, the transfer learning (TL), where the experience that an agent gained to perform one task can be applied in a different task, is limited and the agent performs poorly even on a new but similar task. 

In order to overcome these limitations of existing RL methods, we introduce a new RL approach which is based on using Inductive Logic Programming (ILP). 
ILP is another subfield of machine learning based on logic programming and focuses on the computation of the hypothesis expressed in the form of logic programs that, together with background knowledge, entail all positive examples and none of the negative examples. 
% TODO An ILP system is a rule-based supervised concept learning over examples, and maps xXXXX
ILP has several advantages compared to RL. 
First, unlike most of statistical machine learning methods, ILP requires a small number of training data due to the presence of language bias which defines what the logic program can learn.
Second, the learnt hypothesis is expressed using a symbolic representation and therefore is easy to interpret for human users.
Third, since the learnt hypothesis is a abstract and general concept, it can be easily applied to a different learning task, making transfer learning possible.
The disadvantages of ILP system are that examples, or training data, have to be clearly defined.
Another disadvantage is that ILP suffers from scalability. 
The search space for a hypothesis defined by the language bias increases with respect to the complexity of learning tasks and slows down the learning process.
Despite these shortcomings, however, there has been a number of advance in ILP, especially ILP frameworks based on Answer Sets Programming (ASP), a declarative logic programming which defines semantics based on Herbrand models (Gelfond and Lifschitz 1988).
Because of the progress and limitations of RL and ILP, we developed a new RL approach by incorporating a new ASP-based ILP framework called Learning from Answer Set (ILASP), which learns a valid move within an environment, 
and uses the learnt hypothesis and background knowledge to generate a sequence of actions in the environment. 

In recent RL research, there is a number of research on introducing symbolic representation into RL in order to achieve more data-efficient learning as well as transparent learning. 
One of the methods is to incorporate symbolic representations into the system \cite{Garnelo2016}. This approach is promising and shows potentials.
In addition, there has been works on using ASP into RL to achieve data-efficient learning.
However, none of the papers attempt to apply inductive learning in RL scenarios. 
% but the combining of ILP and RL has not been explored.
% In addition, most of the ILP frameworks are also tested in senarios where the environment is known to the agent in advance.

Since the recent ILP frameworks enable us to learn a complex hypothesis in a more realistic environments, 

Finally, the recent advance of Inductive Logic Programming (ILP) research has enabled us to apply ILP in more complex situations and there are a number of new algorithms based on Answer Set Programmings (ASPs) that work well in non-monotonic scenarios.

Because of the recent advancement of ILP and RL, it is natural to consider that a combination of both approaches would be the next field to explore.
Therefore my motivation is to combine these two different subfields of machine learning and devise a new way of RL algorithm in order to overcome some of the RL problems.
% This new approach is tested in a various
% My motivation is to also test the algorithm where the agent does not know the environment in advance, which is the typical case for RL research.
% Second, it resembles how humans reason. Similar to reinforcement learning, there are some aspects of trial-and-error in human learning, but humans exploit reasonings to efficiently learn about their surrounding or situations. 

Particularly since \cite{Garnelo2016}, there have been several researches that further explored the incorporation of symbolic reasoning into RL, but the combining of ILP and RL has not been explored. 

\section{Objectives}
\label{sec:objectives}

The main objective of this project is to provide a proof-of-concept of a new RL framework using ILP called ILP(RL) and to investigate the potentials of ILP(RL) for improving the learning efficiency and transfer learning capability that current RL methods suffer.
% a ASP-based framework called Inductive Learning of Answer Set Programs (ILASP)
% ILASP is a state-of-art ILP method that can be applied to incomplete and more complex environments.
% This initial proof of concept showed promising preliminary results and as well as limitation of the current framework. 
% Nevertheless, there is avenues for potential improvement, which could be explored in further research.

The objective of this project is divided into the following high-level objectives: 

\newcommand\litem[1]{\item{\bfseries #1.\\}}
\begin{enumerate}
\litem{Translation of state transitions into ASP syntax} 
In RL, an agent interacts with an environment by taking an action and observing a state and rewards (MDP). 
Since ASP-based ILP algorithms require their inputs to be specified in ASP syntax, the conversion between MDP and ASP is required.
\litem{Development of learning tasks} ILP frameworks are based on a search space specified by language bias for a learning task, and need to be specified by the user. 
Various hyper-parameters for the learning task are also considered.
\litem{Using the learnt concept to execute actions}
Having learnt a hypothesis using an ILP algorithm, the agent needs to choose an action based on the learnt hypothesis.
We investigate how the agent can effectively plan a sequence of actions using the hypothesis.
\litem{Evaluation of the new framework in various environments}
In order to investigate the applicability of ILP-based approach, evaluation of the new framework in various scenarios is conducted in order to gain
insight into its potential, especially how it improves the learning process and capability of transfer learning.
\end{enumerate}

% Discrete and deterministic environment. 
This project is a proof of concept for a new way of RL using ILP and therefore more complex environments such as continuous states or stochastic environments are not considered in this project. 
The possibilities of applying these more complex environments are discussed in Section\ref{sec:further_research}.

\section{Contributions}
\label{contributions}
The main contribution of this project is the development of a novel ILP based approach to reinforcement learning .
This project contributes to the incorporation of ASP-based ILP learning frameworks into Reinforcement Learning by applying the latest ILP framework called Learning from Answer Sets. 
To my knowledge, this is the first attempt of incorporating an ASP-based ILP learning framework is incorporated into an RL scenario.
% and the algorithm learns hypotheses, which is the valid move of the game, which is a general concept and therefore can be easily applied to a different scenarios.

In simple environments, we show that an agent learns the rules of the game and reaches an optimal policy faster than existing RL algorithms.
We also show that the learnt hypothesis is easy to understand for human users, and can be applied to other environments to optimise the agent's learning process.

The full hypotheses were learnt in the early stage of the learning and exploration phase. Thus with sufficient exploration, the model of the environment is correct
and therefore the agent is able to find the optimal policy, or the shortest path. 

We show that ILP(RL) is able to solve a reduced MDP where the rewards are assumed to be associated with a sequence of actions planned as answer sets.
Although this is a limited solution, there is a potential to expand it to solve full MDP as discussed in Further Research. 

\textcolor{red}{TODO more details on the strength of the algorithm. Validity}

\section{Report Outline}
\begin{customthm}{2}
The necessary background of Answer Set Programming, Inductive Logic Programming and Reinforcement Learning for this project are described.
\end{customthm}

\begin{customthm}{3}
The descriptions of the new framework, called ILP(RL), is explained in details, and we highlight each aspect of the learning steps with examples. 
\end{customthm}

\begin{customthm}{4}
The performance of ILP(RL) is measured in various maze game environments the learning efficiency and the capability of the transfer learning are compared it against two existing RL algorithms.
We evaluate the outcomes and discuss some of the issues we currently face with the current framework.
\end{customthm}

\begin{customthm}{5}
We review previous research in the related fields. Since there is no research that attempts to apply ASP-based ILP to RL, we review  
applications of ASP in RL and the symbolic representations in RL.  
\end{customthm}

\begin{customthm}{6}
We summarise the framework and experiments of ILP(RL) and discuss avenues of further research. 
\end{customthm}