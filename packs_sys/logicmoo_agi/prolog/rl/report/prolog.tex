
% \usepackage[top=1in]{geometry}
\usepackage{textcomp}
\usepackage{listings}
%\usepackage{minted}      % (requires -shell-escape)
\usepackage{xcolor}
\usepackage{filecontents}

% --- ugly internals for language definition ---
%
\makeatletter

% initialisation of user macros
\newcommand\PrologPredicateStyle{}
\newcommand\PrologVarStyle{}
\newcommand\PrologAnonymVarStyle{}
\newcommand\PrologAtomStyle{}
\newcommand\PrologOtherStyle{}
\newcommand\PrologCommentStyle{}

% useful switches (to keep track of context)
\newif\ifpredicate@prolog@
\newif\ifwithinparens@prolog@

% save definition of underscore for test
\lst@SaveOutputDef{`_}\underscore@prolog

% local variables
\newcount\currentchar@prolog


\newcommand\@testChar@prolog%
{%
  % if we're in processing mode...
  \ifnum\lst@mode=\lst@Pmode%
    \detectTypeAndHighlight@prolog%
  \else
    % ... or within parentheses
    \ifwithinparens@prolog@%
      \detectTypeAndHighlight@prolog%
    \fi
  \fi
  % Some housekeeping...
  \global\predicate@prolog@false%
}



% helper macros
\newcommand\detectTypeAndHighlight@prolog
{%
  % First, assume that we have an atom.
  \def\lst@thestyle{\PrologAtomStyle}%
  % Test whether we have a predicate and modify the style accordingly.
  \ifpredicate@prolog@%
    \def\lst@thestyle{\PrologPredicateStyle}%
  \else
    % Test whether we have a predicate and modify the style accordingly.
    \expandafter\splitfirstchar@prolog\expandafter{\the\lst@token}%
    % Check whether the identifier starts by an underscore.
    \expandafter\ifx\@testChar@prolog\underscore@prolog%
      % Check whether the identifier is '_' (anonymous variable)
      \ifnum\lst@length=1%
        \let\lst@thestyle\PrologAnonymVarStyle%
      \else
        \let\lst@thestyle\PrologVarStyle%
      \fi
    \else
      % Check whether the identifier starts by a capital letter.
      \currentchar@prolog=65
      \loop
        \expandafter\ifnum\expandafter`\@testChar@prolog=\currentchar@prolog%
          \let\lst@thestyle\PrologVarStyle%
          \let\iterate\relax
        \fi
        \advance \currentchar@prolog by 1
        \unless\ifnum\currentchar@prolog>90
      \repeat
    \fi
  \fi
}

\newcommand\splitfirstchar@prolog{}
\def\splitfirstchar@prolog#1{\@splitfirstchar@prolog#1\relax}
\newcommand\@splitfirstchar@prolog{}
\def\@splitfirstchar@prolog#1#2\relax{\def\@testChar@prolog{#1}}

% helper macro for () delimiters
\def\beginlstdelim#1#2%
{%
  \def\endlstdelim{\PrologOtherStyle #2\egroup}%
  {\PrologOtherStyle #1}%
  \global\predicate@prolog@false%
  \withinparens@prolog@true%
  \bgroup\aftergroup\endlstdelim%
}

% language name
\newcommand\lang@prolog{Prolog-pretty}
% ``normalised'' language name
\expandafter\lst@NormedDef\expandafter\normlang@prolog%
\expandafter{\lang@prolog}

% language definition
\expandafter\expandafter\expandafter\lstdefinelanguage\expandafter%
{\lang@prolog}
{%
  language            = Prolog,
  keywords            = {},      % reset all preset keywords
  showstringspaces    = false,
  alsoletter          = (,
  alsoother           = @$,
  moredelim           = **[is][\beginlstdelim{(}{)}]{(}{)},
  MoreSelectCharTable =
    \lst@DefSaveDef{`(}\opparen@prolog{\global\predicate@prolog@true\opparen@prolog},
}

% Hooking into listings to test each ``identifier''
\newcommand\@ddedToOutput@prolog\relax
\lst@AddToHook{Output}{\@ddedToOutput@prolog}

\lst@AddToHook{PreInit}
{%
  \ifx\lst@language\normlang@prolog%
    \let\@ddedToOutput@prolog\@testChar@prolog%
  \fi
}

\lst@AddToHook{DeInit}{\renewcommand\@ddedToOutput@prolog{}}

\makeatother
%
% --- end of ugly internals ---
% --- definition of a custom style similar to that of Pygments ---
% custom colors
\definecolor{PrologPredicate}{RGB}{000,031,255}
\definecolor{PrologVar}      {RGB}{024,021,125}
\definecolor{PrologAnonymVar}{RGB}{000,127,000}
\definecolor{PrologAtom}     {RGB}{186,032,032}
\definecolor{PrologComment}  {RGB}{063,128,127}
\definecolor{PrologOther}    {RGB}{000,000,000}

% redefinition of user macros for Prolog style
\renewcommand\PrologPredicateStyle{\color{PrologPredicate}}
\renewcommand\PrologVarStyle{\color{PrologVar}}
\renewcommand\PrologAnonymVarStyle{\color{PrologAnonymVar}}
\renewcommand\PrologAtomStyle{\color{PrologAtom}}
\renewcommand\PrologCommentStyle{\itshape\color{PrologComment}}
\renewcommand\PrologOtherStyle{\color{PrologOther}}


% custom style definition 
\lstdefinestyle{Prolog-pygsty}
{
  language     = Prolog-pretty,
  upquote      = true,
  stringstyle  = \PrologAtomStyle,
  commentstyle = \PrologCommentStyle,
  literate     =
    {:-}{{\PrologOtherStyle :-}}2
    {,}{{\PrologOtherStyle ,}}1
    {.}{{\PrologOtherStyle .}}1
}

% global settings
\lstset
{
  captionpos = below,
  frame      = single,
  columns    = fullflexible,
  basicstyle = \ttfamily,
}

\begin{filecontents*}{learning_task_example1.pl}
% Background knowledge
cell((0..5, 0..5)).
adjacent(right,(X+1,Y),(X,Y)):-cell((X,Y)),cell((X+1,Y)).
adjacent(left,(X,Y),(X+1,Y)):-cell((X,Y)),cell((X+1,Y)).
adjacent(down,(X,Y+1),(X,Y)):-cell((X,Y)),cell((X,Y+1)).
adjacent(up,(X,Y),(X,Y+1)):- cell((X,Y)),cell((X,Y+1)).
% Context dependent examples 
#pos({state_after((3,4))}, 
     {state_after((2,4)),state_after((1,5)),
     state_after((0,4)),state_after((1,4))}, 
     {state_before((2,4)). action(right). 
     wall((1, 4)). wall((4, 2)).}).
% Language bias 
#modeh(state_after(var(cell))).
#modeb(1, adjacent(const(action),
          var(cell),var(cell)),(positive)).
#modeb(1, state_before(var(cell)), (positive)).
#modeb(1, action(const(action)), (positive)).
#modeb(1, wall(var(cell))).

#max_penalty(50).

#constant(action, right).
#constant(action, left).
#constant(action, down).
#constant(action, up).
\end{filecontents*}

\begin{filecontents*}{learning_task_example2.pl}
#pos({state_after((4,4))},  % Inclusion
     {state_after((4,3)),state_after((3,4)),% Exclusion
     state_after((5,4)),state_after((4,5))}, 
    {state_before((4,4)).action(right). % Context
    wall((5,4)).wall((4,5)).}).
\end{filecontents*}

\begin{filecontents*}{asp_planning.pl}
% Hypotheses are given from ILASP.

% Choice rule for choosing an action at each time T.
1{action(down,T);
  action(up,T);
  action(right,T);
  action(left,T)}1 :-time(T), not finished(T).

% T is the current time step, T_max is the maximum time steps. 
% The agent can do a planning between these time steps.
time(T..T_max).

% Check whether the agent reaches a terminal state.
finished(T):- goal(T2), time(T), T >= T2.
goal(T):- state_at((X_terminal, Y_terminal), T), not finished(T-1).
goalMet:- goal(T).
:- not goalMet.

% walls are cumulatively collected from 
% context dependent examples
wall((X_1, Y_1)).
wall((X_2, Y_2)).
... 

% Current state of the agent at time T, 
% which is the start of the planning
state_at((X_start, Y_start), T).

% The output of ASP should include only state_at and action
#show state_at/2.
#show action/2.

% Find a shortest path to a terminal state, thus
% minimise the number of actions to reach the terminal state.
#minimize{1, X, T: action(X,T)}.

% The size of the maze
cell((0..X_width, 0..Y_height)).

adjacent(right,(X+1,Y),(X,Y)):-cell((X,Y)),cell((X+1,Y)).
adjacent(left,(X,Y),(X+1,Y)):-cell((X,Y)),cell((X+1,Y)).
adjacent(down,(X,Y+1),(X,Y)):-cell((X,Y)),cell((X,Y+1)).
adjacent(up,(X,Y),(X,Y+1)):-cell((X,Y)),cell((X,Y+1)).
      
\end{filecontents*}
  
\begin{filecontents*}{asp_planning_example.pl}
% Learnt hypotheses
state_at(V0, T+1):-time(T),adjacent(right, V0, V1),
                   state_at(V1,T),action(right,T),not wall(V0).
state_at(V0, T+1):-time(T),adjacent(left, V0, V1),
                   state_at(V1,T),action(left,T), not wall(V0).
state_at(V0, T+1):-time(T),adjacent(down, V0, V1), 
                   state_at(V1,T),action(down,T), not wall(V0).
state_at(V0, T+1):-time(T),adjacent(up, V0, V1), 
                   state_at(V1,T),action(up,T), not wall(V0).

1{action(down, T); 
  action(up, T); 
  action(right, T); 
  action(left, T)}1 :- time(T), not finished(T).

% The maximum time step is 4
time(0..4).

finished(T):- goal(T2), time(T), T >= T2.
goal(T):- state_at((3, 1), T), not finished(T-1).
goalMet:- goal(T).
:- not goalMet.

% Walls that the agent knows so far
wall((1, 0)).
wall((2, 0)).
wall((3, 0)).
wall((0, 1)).
wall((0, 2)).
wall((0, 3)).
wall((2, 2)).
wall((3, 2)).

% Starting state
state_at((1, 3), 0).

#show state_at/2.
#show action/2.
#minimize{1, X, T: action(X,T)}.

% Size of the maze
cell((0..4, 0..4)).

adjacent(right, (X+1,Y),(X,Y))   :- cell((X,Y)), cell((X+1,Y)).
adjacent(left,(X,Y),  (X+1,Y)) :- cell((X,Y)), cell((X+1,Y)).
adjacent(down, (X,Y+1),(X,Y))   :- cell((X,Y)), cell((X,Y+1)).
adjacent(up,   (X,Y),  (X,Y+1)) :- cell((X,Y)), cell((X,Y+1)).
\end{filecontents*}

\begin{filecontents*}{vgdl.pl}
  BasicGame
    SpriteSet
        floor > Immovable img=oryx/backBiege
        structure > Immovable
        goal  > color=GREEN img=oryx/door2
        avatar > MovingAvatar img=oryx/mage1
        wall > Immovable img=oryx/dirtWall_0 autotiling=True

    InteractionSet
        random wall structure     > stepBack
        avatar wall      > stepBack
        goal   avatar    > killSprite scoreChange=1
        avatar portalentry > teleportToExit

    TerminationSet
        SpriteCounter stype=goal   limit=0 win=True
        SpriteCounter stype=avatar limit=0 win=False

    LevelMapping
        g > floor goal
        w > floor wall
        A > floor avatar
        + > floor
\end{filecontents*}
  
\begin{filecontents*}{openai.py}
  import gym # import OpenAI gym package
  import gym_vgdl # used to connect VGDL and OpenAI gym

  num_episodes = 100 # num_episodes is specified by users
  time_steps = 100 # time_steps is specified by users

  # initialise an instance of a VDGL game
  env = gym.make('VDGL_ENVNAME')

  for i_episode in range(num_episodes): 
      env.reset()  # the agent starts from a starting point
      for t in range(time_steps): 
          action = 0 # an integer between 0 and 3
                     # and is chosen by your RL algorithm.
                     # Action 0: Up, 1: Down, 2: Left, 3: Right

          # take an action and get an observation
          next_state, reward, done, _ = env.step(action)
          env.render() # update the frame of the environment

          # when done is True, the agent is at a terminal state
          # and the current episode is finished
          if done:
              break
\end{filecontents*}
  

\begin{filecontents*}{experiment1_hypothesis.pl}
state_after(V1):-adjacent(right, V0, V1), state_before(V1), 
                   action(right), wall(V0).
state_after(V0):-adjacent(right, V0, V1), state_before(V0), 
                   action(left), wall(V1).
state_after(V1):-adjacent(down, V0, V1), state_before(V1), 
                   action(down), wall(V0).
state_after(V1):-adjacent(up, V0, V1), state_before(V1), 
                   action(up), wall(V0).
state_after(V0):-adjacent(right, V0, V1), state_before(V1), 
                   action(right), not wall(V0).
state_after(V0):-adjacent(left, V0, V1), state_before(V1), 
                   action(left), not wall(V0).
state_after(V0):-adjacent(down, V0, V1), state_before(V1), 
                   action(down), not wall(V0).
state_after(V0):-adjacent(up, V0, V1), state_before(V1), 
                   action(up), not wall(V0).
\end{filecontents*}

\begin{filecontents*}{experiment1_asp.pl}
state_at((2,5),0), action(right,0)
state_at((3,5),1), action(right,1)
state_at((4,5),2), action(right,2)
state_at((5,5),3), action(right,3)
state_at((6,5),4), action(right,4)
state_at((7,5),5), action(right,5)
state_at((8,5),6), action(right,6)
state_at((9,5),7), action(right,7)
state_at((10,5),8), action(right,8)
state_at((11,5),9), action(right,9)
state_at((12,5),10), action(right,10)
state_at((13,5),11), action(up,11)
state_at((13,4),12), action(up,12)
state_at((13,3),13), action(right,13)
state_at((14,3),14), action(right,14)
state_at((15,3),15), action(right,15)
state_at((16,3),16), action(up,16)
state_at((16,2),17), action(up,17)
state_at((16,1),18)
\end{filecontents*}

\begin{filecontents*}{experiment2_hypothesis_intermediate.pl}
state_after(V1):-link_dest(V1).
state_after(V0):-link_dest(V0), state_before(V0),action(right).
state_after(V1):-adjacent(left, V0, V1), state_before(V0), 
                 action(right), not wall(V1).
state_after(V0):-adjacent(left, V0, V1), state_before(V1), 
                 action(left), not wall(V0).
state_after(V1):-adjacent(up, V0, V1), state_before(V0), 
                 action(down), not wall(V1).
state_after(V0):-adjacent(up, V0, V1), state_before(V1), 
                 action(up), not wall(V0).
state_after(V1):-adjacent(left, V0, V1), state_before(V1), 
                 action(left), wall(V0).
state_after(V1):-adjacent(down, V0, V1), state_before(V1), 
                 action(down), wall(V0).
state_after(V1):-adjacent(up, V0, V1), state_before(V1), 
                 action(up), wall(V0).
\end{filecontents*}

\begin{filecontents*}{experiment2_incorrect_asp.pl}

\end{filecontents*}

\begin{filecontents*}{experiment2_hypothesis.pl}
state_after(V1):-link_start(V0), link_dest(V1), 
                 state_before(V0).
state_after(V1):-adjacent(left, V0, V1), state_before(V0),
                 action(right), not wall(V1).
state_after(V0):-adjacent(left, V0, V1), state_before(V1),
                 action(left), not wall(V0).
state_after(V1):-adjacent(up, V0, V1), state_before(V0),
                 action(down), not wall(V1).
state_after(V0):-adjacent(up, V0, V1), state_before(V1),
                 action(up), not wall(V0).
state_after(V0):-adjacent(left, V0, V1), state_before(V0),
                 action(right), wall(V1).
state_after(V1):-adjacent(left, V0, V1), state_before(V1),
                 action(left), wall(V0).
state_after(V0):-adjacent(up, V0, V1), state_before(V0),
                 action(down), wall(V1).
state_after(V1):-adjacent(up, V0, V1), state_before(V1),
                 action(up), wall(V0).
\end{filecontents*}

\begin{filecontents*}{experiment2_asp.pl}
state_at((2,5),0), action(right,0)
state_at((3,5),1), action(right,1)
state_at((4,5),2), action(right,2)
state_at((5,5),3), action(right,3)
state_at((6,5),4), action(right,4)
state_at((7,5),5), action(right,5)
state_at((8,5),6), action(right,6)
state_at((9,5),7), action(right,7)
state_at((10,5),8), action(down,8)
state_at((10,6),9), action(down,9)
state_at((10,7),10), state_at((13,1),10), action(right,10)
state_at((10,7),11), state_at((14,1),11), action(right,11)
state_at((10,7),12), state_at((15,1),12), action(right,12)
state_at((10,7),13)
state_at((16,1),13)
\end{filecontents*}

\begin{filecontents*}{experiment3_hypothesis.pl}
state_after(V0):-adjacent(right, V0, V1), state_before(V1), 
                 action(right), not wall(V0).
state_after(V0):-adjacent(left, V0, V1), state_before(V1), 
                 action(left), not wall(V0).
state_after(V1):-adjacent(down, V0, V1), state_before(V0), 
                 action(up), not wall(V1).
state_after(V0):-adjacent(down, V0, V1), state_before(V1), 
                 action(down), not wall(V0).
state_after(V1):-adjacent(right, V0, V1), state_before(V1), 
                 action(right), wall(V0).
state_after(V1):-adjacent(left, V0, V1), state_before(V1), 
                 action(left), wall(V0).
state_after(V0):-adjacent(up, V0, V1), state_before(V0), 
                 action(down), wall(V1).
state_after(V1):-adjacent(up, V0, V1), state_before(V1), 
                 action(up), wall(V0).
\end{filecontents*}
  
\begin{filecontents*}{experiment4_hypothesis.pl}
state_after(V1):-link_start(V0), link_dest(V1), 
                 state_before(V0).
state_after(V1):-adjacent(left, V0, V1), state_before(V0), 
                 action(right), not wall(V1).
state_after(V0):-adjacent(left, V0, V1), state_before(V1), 
                 action(left), not wall(V0).
state_after(V1):-adjacent(up, V0, V1), state_before(V0), 
                 action(down), not wall(V1).
state_after(V0):-adjacent(up, V0, V1), state_before(V1), 
                 action(up), not wall(V0).
state_after(V0):-adjacent(left, V0, V1), state_before(V0), 
                 action(right), wall(V1).
state_after(V1):-adjacent(left, V0, V1), state_before(V1), 
                 action(left), wall(V0).
state_after(V0):-adjacent(up, V0, V1), state_before(V0), 
                 action(down), wall(V1).
state_after(V1):-adjacent(up, V0, V1), state_before(V1), 
                 action(up), wall(V0).  
\end{filecontents*}

\begin{filecontents*}{appendix_learning_task_eval1.pl}
cell((0..18, 0..8)).
adjacent(right, (X+1,Y),(X,Y)):-cell((X,Y)), cell((X+1,Y)).
adjacent(left,(X,Y),  (X+1,Y)):-cell((X,Y)), cell((X+1,Y)).
adjacent(down, (X,Y+1),(X,Y)):-cell((X,Y)), cell((X,Y+1)).
adjacent(up,   (X,Y),  (X,Y+1)):-cell((X,Y)), cell((X,Y+1)).
#modeh(state_after(var(cell))).
#modeb(1,adjacent(const(action),var(cell),var(cell)),
      (positive)).
#modeb(1, state_before(var(cell)),(positive)).
#modeb(1, action(const(action)),(positive)).
#modeb(1, wall(var(cell))).
#max_penalty(50).
#constant(action, right).
#constant(action, left).
#constant(action, down).
#constant(action, up).
% Context dependent examples are added here
\end{filecontents*}

\begin{filecontents*}{appendix_learning_task_eval2.pl}
cell((0..18, 0..8)).
adjacent(right, (X+1,Y),(X,Y)):-cell((X,Y)), cell((X+1,Y)).
adjacent(left,(X,Y),  (X+1,Y)):-cell((X,Y)), cell((X+1,Y)).
adjacent(down, (X,Y+1),(X,Y)):-cell((X,Y)), cell((X,Y+1)).
adjacent(up,   (X,Y),  (X,Y+1)):-cell((X,Y)), cell((X,Y+1)).

#modeh(state_after(var(cell))).
#modeb(1,adjacent(const(action),var(cell),var(cell)),
      (positive)).
#modeb(1,state_before(var(cell)),(positive)).
#modeb(1,action(const(action)),(positive)).
#modeb(1,wall(var(cell))).
% Two additional language biases
#modeb(1, link_start(var(cell)), (positive)).
#modeb(1, link_dest(var(cell)), (positive)).
#max_penalty(50).
#constant(action, right).
#constant(action, left).
#constant(action, down).
#constant(action, up).
#pos({state_after((2,6))}, 
% Context dependent examples are added here
\end{filecontents*}

\begin{filecontents*}{appendix_asp.pl}
1{action(down,T);
  action(up,T);
  action(right,T);
  action(left,T);}1:-time(T), not finished(T).

state_at((1,4),3).
finished(T):-goal(T2),time(T), T >= T2.
goal(T):-state_at((5, 1),T), not finished(T-1).
goalMet:-goal(T).
:-not goalMet.

time(0..30).
cell((0..6, 0..5)).

adjacent(right,(X+1,Y),(X,Y)):-cell((X,Y)),cell((X+1,Y)).
adjacent(left,(X,Y),(X+1,Y)):-cell((X,Y)), cell((X+1,Y)).
adjacent(down,(X,Y+1),(X,Y)):-cell((X,Y)), cell((X,Y+1)).
adjacent(up,(X,Y),(X,Y+1)):-cell((X,Y)), cell((X,Y+1)).

#minimize{1,X,T: action(X,T)}.
#show state_at/2.
#show action/2.
\end{filecontents*}